# -*- coding: utf-8 -*-
"""Homework-6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocudLMTKu8HEkg4NqHiJEFyOcmBUmp1h

# Assignment 6
"""

#Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')
pd.set_option("display.max_columns", 101)

"""###Data Description

Column | Description
:---|:---
`id` | Record index
`timestamp` | Datetime (YYYY:MM:DD HH:MM:SS) when data was collected
`country` | Current country of employment
`employment_status` | Whether a candidate is Full time, Part time, Independent or freelancer or company owner
`job_title` | Current job title of the candidate
`job_years` | Total job experience (in Years)
`is_manager` | Whether the candidate holds a managerial position or not (Yes or No)
`hours_per_week` | No. of hours per day committed to the current job
`telecommute_days_per_week` | No. of telecommuting days per week (working from home)
`education` | The highest degree in education the candidate has received
`is_education_computer_related` | Is the education related to the field of computer science (Yes or No)
`certifications` | Does the candidate have any relevant certifications (Yes or No)
`salary` | Monthly Salary (in US $$)
"""

# Dataset is already loaded below
data = pd.read_csv("employee.csv")

# Dimensions of training data
data.shape

# Print first few rows of data
data.head()

# drop id, timestamp and country columns
data = data.drop(columns=['id', 'timestamp','country'])

# Explore columns
data.columns

# replace NANs in hours_per_week with median value of the column
data.loc[data['hours_per_week'].isna(), 'hours_per_week'] = data['hours_per_week'].median()
data.loc[data['telecommute_days_per_week'].isna(), 'telecommute_days_per_week'] = data['telecommute_days_per_week'].median()

#Handling null values in categorical columns
data = data.dropna()

data.info()

"""###Data Visualization :

## Feature Encoding and Normalization

Before training the model, we should perform one-hot encoding for all categorical/discrete variables, normalize continuous variables and then combine all data to form the training set.
"""

# create another copy of dataset and append encoded features to it
data_train = data.copy()
data_train.head()

# select categorical features
cat_cols = [c for c in data_train.columns if data_train[c].dtype == 'object'
            and c not in ['is_manager', 'certifications']]
cat_data = data_train[cat_cols]
cat_cols

#Encoding binary variables
binary_cols = ['is_manager', 'certifications']
for c in binary_cols:
    data_train[c] = data_train[c].replace(to_replace=['Yes'], value=1)
    data_train[c] = data_train[c].replace(to_replace=['No'], value=0)

final_data = pd.get_dummies(data_train, columns=cat_cols, drop_first= True,dtype=int)
final_data.shape

final_data.columns

final_data

"""## Train Test Split"""

y = final_data['salary']
X = final_data.drop(columns=['salary'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print("Training Set Dimensions:", X_train.shape)
print("Validation Set Dimensions:", X_test.shape)

"""## Pre-processing data

### Standardization (Z-score normalization):

$$ x_{\text{std}} = \frac{x - \mu}{\sigma} $$

- **Purpose:** Standardization transforms the data to have a mean of 0 and a standard deviation of 1.
- **Properties:**
  - Centers the data around 0.
  - Rescales the data to have unit variance.
  - Does not bound the data within a specific range.
  - Preserves the shape of the distribution.
- **Use Cases:**
  - Algorithms that assume zero-centered data or require features to have a similar scale (e.g., gradient descent-based algorithms, support vector machines).
  - When the distribution of the features is Gaussian-like.

### Normalization (Min-Max scaling):

 $$ x_{\text{norm}} = \frac{x - \min(x)}{\max(x) - \min(x)} $$

- **Purpose:** Normalization scales the data to a fixed range, typically [0, 1] or [-1, 1].
- **Properties:**
  - Scales the data to a specified range.
  - Shifts the data to start at 0.
  - Does not affect the shape of the distribution.
  - Preserves the relative relationships between data points.
- **Use Cases:**
  - Neural networks, especially those with activation functions sensitive to input magnitudes (e.g., sigmoid or tanh functions).
  - When the distribution of the features is unknown or non-Gaussian.

**Choosing Between Standardization and Normalization:**
- Use standardization when the distribution of your features is approximately Gaussian-like and you want to center the data and rescale it to have unit variance.
- Use normalization when the scale of your features is important, or when you need to bound the features within a specific range.
- It's often beneficial to try both preprocessing techniques and evaluate their effects on model performance to determine which one works best for your specific dataset and model.
"""

# select numerical features
num_cols = ['job_years','hours_per_week','telecommute_days_per_week']
num_cols

# Apply standard scaling on numeric data
scaler = StandardScaler()
scaler.fit(X_train[num_cols])
X_train[num_cols] = scaler.transform(X_train[num_cols])

X_train

#Fitting a Linear Regression Model
reg=LinearRegression()
reg.fit(X_train, y_train)

reg.coef_

reg.intercept_

"""Just to recall

$\hat{y} = \alpha + \beta_1 * X_1 + \beta_2 * X_2 +...$

Our Final model is given by -

$\hat{y} = 6145.79 + 1.887 * X_1 + 7.22 * X_2 +...$

"""

# Normalized MSE (Dividing by mean)
mean_squared_error(y_train,reg.predict(X_train))/np.mean(y_train)

# Predict on the test data
y_pred = reg.predict(X_test)

#Evaluate the model on test data
mse = mean_squared_error(y_pred, y_test)/np.mean(y_test)
print("Mean Squared Error:", mse)

"""*A lower MSE indicates that the model's predictions are closer to the actual values on average, while a higher MSE suggests larger errors between predictions and actual values.
‚ùóPre-processing on Test data not done.
"""

#Q1. Preprocess Test data and get predictions

# Import necessary libraries
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# Assuming your data is split into `X_test` and `y_test` from the previous steps

# Preprocessing Test Data
# Apply the same scaling on the test data as was done on the training data
scaler = StandardScaler()
scaler.fit(X_train[num_cols])  # Use the scaler fitted on the training set
X_test[num_cols] = scaler.transform(X_test[num_cols])

# Predict on the test data
y_pred = reg.predict(X_test)

# Evaluate the model on test data
mse = mean_squared_error(y_test, y_pred) / np.mean(y_test)
print("Mean Squared Error on Test Data:", mse)

# Load the test data
test_data = data.copy()  # Assuming test data has a similar structure as training data

# Drop unnecessary columns (only if they exist)
columns_to_drop = ['id', 'timestamp', 'country']
test_data = test_data.drop(columns=[col for col in columns_to_drop if col in test_data.columns])

# Replace NaNs in numeric columns
test_data['hours_per_week'].fillna(test_data['hours_per_week'].median(), inplace=True)
test_data['telecommute_days_per_week'].fillna(test_data['telecommute_days_per_week'].median(), inplace=True)

# Drop rows with any remaining missing values
test_data = test_data.dropna()

# Encoding categorical and binary columns for test data
for col in binary_cols:
    test_data[col] = test_data[col].replace(to_replace=['Yes'], value=1)
    test_data[col] = test_data[col].replace(to_replace=['No'], value=0)

test_data = pd.get_dummies(test_data, columns=cat_cols, drop_first=True, dtype=int)

# Scale numeric features in the test set
test_data[num_cols] = scaler.transform(test_data[num_cols])  # Using the scaler fitted on the training data

# Ensure test data columns match the training data by adding any missing columns with zeros
for col in X_train.columns:
    if col not in test_data:
        test_data[col] = 0
test_data = test_data[X_train.columns]  # Align columns order with X_train

# Predict using the trained model
test_predictions = reg.predict(test_data)
test_predictions

# 1. Distribution of numeric features
numeric_features = ['job_years', 'hours_per_week', 'telecommute_days_per_week']
fig, axes = plt.subplots(1, len(numeric_features), figsize=(15, 5))

for i, feature in enumerate(numeric_features):
    sns.histplot(data[feature], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {feature}')

plt.tight_layout()
plt.show()

# 2. Categorical Features Count Plot
fig, axes = plt.subplots(len(cat_cols), 1, figsize=(8, len(cat_cols) * 4))

for i, feature in enumerate(cat_cols):
    sns.countplot(data=data, x=feature, ax=axes[i])
    axes[i].set_title(f'Count of {feature}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 3. Actual vs. Predicted Salaries (Model Performance)
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Actual vs Predicted Salary')
plt.show()

# 4. Residual Plot
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.xlabel('Residuals')
plt.title('Distribution of Residuals')
plt.show()